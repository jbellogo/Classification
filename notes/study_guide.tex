\documentclass{article}

\usepackage{amsfonts}

\usepackage{graphicx}
\graphicspath{ {./images/} }

\usepackage{bm}

\usepackage{float}
\newtheorem{theorem}{Theorem}





\usepackage{amsmath}

\usepackage[a4paper, total={6in, 8in}]{geometry}


\setlength{\parindent}{0pt} % for getting rid of first line indentation





% ================================================
% =====================  BEGIN DOCUMENT 
% ================================================

\title{STAT 441 Study}
\author{Juan Pablo Bello Gonzalez}
\date{September 2024}

\begin{document}

\maketitle

\newpage

\section{1. Learning Concepts}
Learning:
\begin{itemize}
\item Classification: Categorical outcome
\item Regression: Continuous outcome
\end{itemize}

Supervised: You have labels, ie right answers/confirmation for training data. Easier models than unsupervised. In unsupervised you have to make up distinctive features/categories, you don't even know how many clusters there are.

\textbf{Interpretation vs. prediction tradeoff}:
Complex models like trees and neural netoworks are powerful at predicting but there is little interpretability to what the model coefficients actually mean. 

You can restrict the class of models to allow for easy interpretation, ie trees with tunned hyperparameters. THEN optimize for prediction wihtin this class models. 
\subsection{Loss}
MSE

\subsection{Bias-Variance Tradeoff}
MSE loss can be decomposed into three sources of error: \textbf{Bias, Variance, and Irreducible error} 
\[ E [ (y_0 - \hat f(x_0))^2]
= Var(\hat f(x_0))) + [Bias(\hat f(x_0)))]^2 + Var(\epsilon)
\]
All three are non-negarive. 
\begin{itemize}
\item Variance: How well the model generalizes to other data sets. 
\item Bias: how well the model fits the training set.
\end{itemize}

Overfitting: low bias high variance. Perfect fit  from highly flexible model, usually with many hyperparameters, means that the model will have high variance and not predict other test sets as well as it predicts that one. 

Explain the tradeoff:
- Conflict of trying to simulataneously minimize the two sources of error that prevent supervised learning algorithms from generalizing beyond the training set. 

MSE is the sum of three things, the graph of MSE makes a U, and the graphs of its three components are
\begin{itemize}
\item Irreducible error/Var(epsilon). Constant
\item Variance. Increasing with flexibility. The more flexibility, the worse predictive power to  data sets outside the training set the model will have.
\item Bias. Decaying as flexibility increases. Higher flexibility $\implies$ lower bias $\implies$  better fit. 
\end{itemize}

\subsection{Bayes Classifier}
A classification algorithm takes in a unit with features (a row on a table) and gives a probability distribution over the categories. Baye's classifier says, classify the unit with the most likely class.

\begin{theorem}
\textbf{Conditional Bayes Error Rate}: For a given obervation $x$, is $1-max_{j} P(Y =j |x)$ the complement of the Bayes classifier probability. 
\end{theorem}

\begin{theorem}
\textbf{Overall Bayes Error Rate}:  $1-E[max_{j} P(Y =j |x)]$ the EXPECTATION complement of the Bayes classifier probability. 
\end{theorem}

\section{2. Practical aspects}

\subsection{overfitting}
\textbf{Definition:} the model fits the \textbf{random noise} of a \textbf{sample} rather than the generalizable relationship.

Occurs when the model is \textbf{too flexible}. Has too many parameters relative to the number of observations. Advanced learning algorithms are flexible, you don't need a neural network to predict y based on two features, just use some regression.

\subsubsection{Defending against overfitting}
\begin{itemize}
\item Fit to training set
\item Evaluate on test set
\item split must be chosen AT RANDOM. but fix seed in practice.
\end{itemize}


By splitting the data into test/train, overfitting can be avoided. If you train on 100\% of the data there will be overfitting!
We build a model based on the training data but evaluate the model on the test data. 

\subsection{cross-validation}
Split data into k random subsets of equal size, called folds, then fit a model to each of the possible combinations of k-1 folds and evaluate/test on the remaining fold. Average accuracty scores to obtain a good estimate of the true predictive power of the model.

In extreme case: leave-one-out (LOO) cross validation. It's nonsense for most applications.

\subsection{Evaluation measures}
\begin{itemize}
\item Accuracy
\item Sensitivity 
\item Specificity
\item Area under the ROC curve
\item F
\end{itemize}

for classification, consider: 
\begin{itemize}
\item TP 
\item FP
\item RN
\item FN
\end{itemize}

the ordering in the name is (Correct label?, model classification label). Correct label? means that those that start with a true: TP, TN were correctly classified. The seonc instance is what that label classification was duh.

\textbf{Confusion Matrix}
\[
\begin{array}{|c|c|c|}
\hline
 & \text{Predicted Positive} & \text{Predicted Negative} \\
\hline
\text{Actual Positive} & TP & FN \\
\hline
\text{Actual Negative} & FP & TN \\
\hline
\end{array}
\]

\subsubsection{Accuracy, Sensitivty, and Specificity}
some formulas for evaluation measures
\begin{itemize}
\item Accuracy: (TP+TN)/N, where N is the sum of all four = \#observations or sample size. Also Accuracy = $\frac{1}{n} \sum_{i=1}^n I(y_i = \hat y_i)$
\item Sensitivity (TRUE POS rate): TP/(TP + FN)
\item Specificity (TRUE NEG rate): TN/(TN+ FP)
\end{itemize}

Sensitivity is positive. 
$1-\text{Specificity}$ is False positive rate. 

Ideally we want high specificty and sensitivity.But a trade off needs to be made. The threshold of prediction... you may want to be conservative in your true predictions (high specificity low sensitiviy) if it is for Cancer diagnosis. But liberal if it is about who gets on a survival boat.

\subsubsection{ROC}
Area under curve is a measure of the sensitivity/specificity tradeoff. 

\subsubsection{F-measure}
\[
F = \frac{2 \times TP}{2 \times TP + FP +FN}
\]

\begin{itemize}
\item Higher values are better. 
\item appropiate for 0/1 classification.
\end{itemize}

Important: 
- Sometimes, depending on the context, you may want to always predict false. An example: when there are ratre or uncommon events. 
- F measure is affected the most by this. Even if you don't predict false all the time, as long as there is a clear bias in the distribution of false classifications, your TP rate will be very low and so your F-measure will be small as well. 


\subsubsection{Dealing with rarely occuring categories}
Not uncommon to have rare categories, ie. categories that occur infrequently.
\begin{itemize}
\item Macro averaging: Treat all categories the same. Default of all algorithms. Default of not doing anything in preprocessing.
\item \textbf{MICRO} averaging: dominate by frequent categories, giving little weight to rare categories.
\end{itemize}


\subsubsection{One-hot-encoding or "factoring" in R}
Sometimes, variates are given in terms of categories themselves like X2 = "Family role" $\in$ [1 : "Father", 2: "son", 3:"sibling"]. \\

We shouldn't just feed these categories as they are to an algorithm because the scale and ordering provided by the category numbers, $[1,2,3]$ in the prior example, doesn't mean anything real, and could at best add noise to the data, and at worse confoundingly influence the predictions.\\ 

So for a categorical variate with $n$ categories, we take $n-1$ indicator variables to represent the presence of that category in that variate, this is sometimes known as factoring. \\

why not do $n$ indicators, one for each category number? Because that would cause multicolinearity. As the $n-$th indicator is just the absence of the $n-1$ other ones (you can write them in a linear dependence)

\textbf{NOTE:} We often don't leave-one-out in advanced statistical models because they don't always use all variates, or collinearity doesn't affect the model like in regression, where you absoluteley cannot have dependencies between variates.


\subsubsection{Variable scaling}
Do when variables have vastly differnt ranges, helps give bettwr results by making variates more comparable to each other. 
Scaling is also called standardization.

Scalling methods:
\begin{itemize}
\item Scale them to have mean zero and stdev =1 using $x_{scaled} = \frac{x-\bar x}{sd_x}$. Note that indicator variables are not scaled like this, their mean and variance may not be 0,1.
\item Scale variables to have the same range.
\end{itemize}








\end{document}
